{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of Random Forest classifier\n",
    "\n",
    "In the following, we evaluate different versions of the Rhapsody classifier, through 10-fold cross-validation over the Integrated Dataset, in order to find optimal parameters. More specifically, we considered:\n",
    "* different subsets of the training dataset\n",
    "* different subsets of features\n",
    "* different classifier's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from prody import LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.family': 'Arial'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, insert here local path to Rhapsody folder with the command:\n",
    "# sys.path.insert(0, '/LOCAL_PATH/rhapsody/')\n",
    "import rhapsody as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('local'):\n",
    "    os.mkdir('local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training dataset\n",
    "\n",
    "The Integrated Dataset used for training is available as a NumPy structured array containing all precomputed features, as well as true labels and other info (e.g. PDB lengths).\n",
    "\n",
    "The Integrated Dataset has been built by merging 7 publicly available datasets of labelled human missense variants (HumVar, ExoVar, predictSNP, VariBench, SwissVar, Humsavar, ClinVar). Variants with discordant interpretations between datasets have `true_label = -1`. See `Training_Dataset` tutorial for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make sure to extract the data folder beforehand\n",
    "ID = np.load('../0-Training_Dataset/local/data/precomputed_features-ID.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array structure\n",
    "print(ID.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each entry can be accessed by indexing\n",
    "ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the training dataset\n",
    "len(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's discard SAVs with unknown significance (true_label == -1)\n",
    "ID = ID[ID['true_label'] != -1]\n",
    "len(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries with an associated PDB structure\n",
    "len(ID[ ID['PDB_length' ] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset filtering based on ClinVar review status\n",
    "\n",
    "Each entry in the ClinVar dataset is assigned with a review \"star\", between 0 to 4, which indicates the quality of the clinical interpretation, based on consensus between sources. \n",
    "\n",
    "We start by analyzing the impact on the classifier's accuracy of increasingly strict constraints on SAVs quality (at least for SAVs found in the ClinVar database).\n",
    "\n",
    "For this analysis, we will make a couple of reasonable assumptions that will be justified *a posteriori* in the next sections, for instance:\n",
    "* we will discard SAVs mapped to PDB structures smaller than 150 residues\n",
    "* we will select a specific set of features (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_SAVs_info = np.load('../0-Training_Dataset/local/data/Integrated_Dataset-SAVs.npy')\n",
    "ID_SAVs_info.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(ID_SAVs_info['ClinVar_review_star'])\n",
    "\n",
    "print('review star   n. SAVs')\n",
    "for star in range(5):\n",
    "    print(f'{star:>11} {count[star]:>9}')\n",
    "    \n",
    "print(f'SAVs without ClinVar review star: {count[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClinVar_review_stars = {s['SAV_coords']: s['ClinVar_review_star'] for s in ID_SAVs_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featset = rd.DEFAULT_FEATSETS['full']\n",
    "featset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('local/results'):\n",
    "    os.mkdir('local/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('local/results/CV_summaries_1.pkl'):\n",
    "    CV_summaries_1 = pickle.load(open('local/results/CV_summaries_1.pkl', 'rb'))\n",
    "    print('A pickle containing precomputed results has been found. \\n'\n",
    "          'Please delete it if you wish to run the analysis again. \\n'\n",
    "          'Log saved in local/results/RF_optimization_1.log \\n')\n",
    "    ! head -25 local/results/RF_optimization_1.log; echo '...'\n",
    "else:\n",
    "    # let's consider only PDBs larger than 150 residues\n",
    "    ID_150 = ID[ ID['PDB_length'] >= 150 ]\n",
    "\n",
    "    # cycle of cross-validation tests after excluding ClinVar SAVs \n",
    "    # with increasing reliability ranking (= review star) from training dataset\n",
    "\n",
    "    CV_summaries_1 = {}\n",
    "    \n",
    "    LOGGER.start('local/results/RF_optimization_1.log')\n",
    "\n",
    "    for star in range(6):\n",
    "        \n",
    "        # we exclude SAVs with review star smaller than given value \n",
    "        # (if ClinVar's rating is found)\n",
    "        ID_subset = ID_150[ [ClinVar_review_stars[SAV]==-1 or \n",
    "                             ClinVar_review_stars[SAV] >= star \n",
    "                             for SAV in ID_150['SAV_coords']] ]\n",
    "\n",
    "        LOGGER.info(f'REVIEW STAR: {star:1}')\n",
    "        LOGGER.info(f'SAVs removed from ID: {len(ID_150)-len(ID_subset):4}')\n",
    "\n",
    "        # create folder\n",
    "        folder = f'local/results/review_star-{star}'\n",
    "        os.mkdir(folder)\n",
    "        \n",
    "        # run cross-validation\n",
    "        sel = ['SAV_coords', 'true_label'] + featset\n",
    "        ID_subset = ID_subset[sel]\n",
    "        CV_summaries_1[star] = rd.RandomForestCV(ID_subset)\n",
    "\n",
    "        # move figures into folder\n",
    "        for file in glob('*png'):\n",
    "            os.rename(file, os.path.join(folder, file))\n",
    "\n",
    "        LOGGER.info('')\n",
    "\n",
    "    # store all cross-validation results into a pickle\n",
    "    pickle.dump(CV_summaries_1, open('local/results/CV_summaries_1.pkl', 'wb'))\n",
    "\n",
    "    LOGGER.close('local/results/RF_optimization_1.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('local/figures'):\n",
    "    os.mkdir('local/figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5,4))\n",
    "\n",
    "x = range(6)\n",
    "\n",
    "y = np.array([CV_summaries_1[n]['mean AUROC'][0] for n in x])\n",
    "e = np.array([CV_summaries_1[n]['mean AUROC'][1] for n in x])\n",
    "ax.errorbar(x, y, yerr=e, marker='o', capsize=2, label='AUROC')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "hb = ax2.bar(x, [CV_summaries_1[n]['dataset size'] for n in x], \n",
    "             color = 'C8', width=.7, alpha=0.5, label='dataset size')\n",
    "ax2.set_zorder(ax.get_zorder()-1)\n",
    "ax.patch.set_visible(False)\n",
    "\n",
    "ax.set_xlabel('min. ClinVar review star')\n",
    "ax.set_ylabel('AUROC')\n",
    "ax.set_ylim((.84, .875))\n",
    "ax.locator_params(nbins=5, axis='y')\n",
    "\n",
    "ax2.set_ylim((0, 22000))\n",
    "ax2.set_yticks(range(4000, 20001, 4000))\n",
    "ax2.set_yticklabels(['4k\\nSAVs', '8k\\nSAVs', '12k\\nSAVs', '16k\\nSAVs', '20k\\nSAVs'])\n",
    "\n",
    "ax.legend(handlelength=1, loc='upper left')\n",
    "ax2.legend(handlelength=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('local/figures/accuracy_vs_review_star.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we will exclude from our training dataset SAVs with a ClinVar review star = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ID = ID\n",
    "\n",
    "ID = ID[ [ClinVar_review_stars[SAV] != 0 for SAV in ID['SAV_coords']] ]\n",
    "print('original dataset size:', len(original_ID))\n",
    "print('filtered dataset size:', len(ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-validation with various classification schemes\n",
    "\n",
    "Next, we test different combinations of features and training datasets, specifically:\n",
    "\n",
    "* different subsets of the training dataset, obtained by setting a minimum PDB structure size\n",
    "* 4 different feature sets, including one reproducing version 1 of the method (RAPSODY)\n",
    "* **GNM** vs **ANM** features\n",
    "* ENM features computed with and without the inclusion of *enviromental* effects (**chain**, **reduced** or **sliced** models), i.e. the presence of other chains in the PDB structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this cell requires a few hours to complete\n",
    "\n",
    "if os.path.isfile('local/results/CV_summaries_2.pkl'):\n",
    "    CV_summaries_2 = pickle.load(open('local/results/CV_summaries_2.pkl', 'rb'))\n",
    "    print('A pickle containing precomputed results has been found. \\n'\n",
    "          'Please delete it if you wish to run the analysis again. \\n'\n",
    "          'Recovered output: \\n')\n",
    "    ! cat local/results/RF_optimization_2.log\n",
    "else:\n",
    "    LOGGER.start('local/results/RF_optimization_2.log')\n",
    "    CV_summaries_2 = {}\n",
    "    \n",
    "    for min_num_res in [1, 100, 150, 200, 300, 400, 500, 600]:\n",
    "        # compute subset of the training dataset\n",
    "        ID_subset = ID[ ID['PDB_length'] >= min_num_res ]\n",
    "        \n",
    "        # loop over different classification schemes\n",
    "        for ENM in ['GNM', 'ANM']:\n",
    "            for model in ['chain', 'reduced', 'sliced']:\n",
    "                for version in ['full', 'reduced', 'EVmut', 'v1']:\n",
    "\n",
    "                    full_featset = [\n",
    "                        'wt_PSIC', 'Delta_PSIC', 'SASA', f'{ENM}_MSF-{model}',\n",
    "                        f'{ENM}_effectiveness-{model}', f'{ENM}_sensitivity-{model}',\n",
    "                        f'stiffness-{model}', 'entropy', 'ranked_MI', 'BLOSUM']\n",
    "                    \n",
    "                    # select feature set (+ true label)\n",
    "                    if version == 'full':\n",
    "                        featset = full_featset\n",
    "                    elif version == 'reduced':\n",
    "                        full_featset.remove('entropy')\n",
    "                        full_featset.remove('ranked_MI')\n",
    "                        featset = full_featset\n",
    "                    elif version == 'EVmut':\n",
    "                        full_featset.append('EVmut-DeltaE_epist')\n",
    "                        # full classifier + EVmutation epistatic score\n",
    "                        featset = full_featset\n",
    "                    elif version == 'v1' and ENM == 'GNM' and model == 'chain':\n",
    "                        # classifier as in version 1 of Rhapsody (RAPSODY)\n",
    "                        # NB: RAPSODY used a combination of GNM/ANM features, which\n",
    "                        # we reproduce here for the sake of comparison\n",
    "                        featset = [\n",
    "                            'wt_PSIC', 'Delta_PSIC', 'SASA', 'GNM_MSF-chain',\n",
    "                            'ANM_effectiveness-chain', 'ANM_sensitivity-chain',\n",
    "                            'stiffness-chain']\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    if version == 'v1':\n",
    "                        scheme = f'{min_num_res}-v1'\n",
    "                    else:\n",
    "                        scheme = f'{min_num_res}-{ENM}-{model}-{version}'\n",
    "                        \n",
    "                    LOGGER.info(f'CLASSIFICATION SCHEME: {scheme}')\n",
    "\n",
    "                    # create folder\n",
    "                    folder = f'local/results/clsf_scheme-{scheme}'\n",
    "                    os.mkdir(folder)\n",
    "                    \n",
    "                    # run cross-validation\n",
    "                    sel = ['SAV_coords', 'true_label'] + featset\n",
    "                    CV_summaries_2[scheme] = rd.RandomForestCV(ID_subset[sel])\n",
    "\n",
    "                    # move figures into folder\n",
    "                    for file in glob('*png'):\n",
    "                        os.rename(file, os.path.join(folder, file))\n",
    "                    \n",
    "                    LOGGER.info('')\n",
    "                    \n",
    "    # store all cross-validation results into a pickle\n",
    "    pickle.dump(CV_summaries_2, open('local/results/CV_summaries_2.pkl', 'wb'))\n",
    "\n",
    "    LOGGER.close('local/results/RF_optimization_2.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures & summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "ax[0,0].set_title('full classifier')\n",
    "ax[0,1].set_title('reduced classifier')\n",
    "ax[0,2].set_title('full+EVmutation classifier')\n",
    "ax[0,0].set_ylabel('AUROC')\n",
    "ax[1,0].set_ylabel('OOB score')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([.815, .87])\n",
    "    ax[1,j].set_ylim([.78, .88])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "   \n",
    "x = [1, 100, 150, 200, 300, 400, 500, 600]\n",
    "for ENM in ['GNM', 'ANM']:\n",
    "    col = 'C0' if ENM=='GNM' else 'C3'\n",
    "    for model in ['chain', 'reduced', 'sliced']:\n",
    "        if model == 'chain':\n",
    "            mk = 'o'\n",
    "        elif model == 'reduced':\n",
    "            mk = '^'\n",
    "        else:\n",
    "            mk = 'v'\n",
    "        for i, version in enumerate(['full', 'reduced', 'EVmut']):\n",
    "            scheme = f'{ENM}-{model}-{version}'\n",
    "            AUC = [CV_summaries_2[f'{n}-{scheme}']['mean AUROC'][0] for n in x]\n",
    "            OOB = [CV_summaries_2[f'{n}-{scheme}']['mean OOB score'][0] for n in x]\n",
    "            if scheme.startswith('ANM-chain'):\n",
    "                AUC_e = [CV_summaries_2[f'{n}-{scheme}']['mean AUROC'][1] for n in x]\n",
    "                OOB_e = [CV_summaries_2[f'{n}-{scheme}']['mean OOB score'][1] for n in x]\n",
    "            else:\n",
    "                AUC_e = None\n",
    "                OOB_e = None\n",
    "            ax[0,i].errorbar(x, AUC, yerr=AUC_e, capsize=2, c=col, m=mk, ms=7)\n",
    "            ax[1,i].errorbar(x, OOB, yerr=OOB_e, capsize=2, c=col, m=mk, ms=7, \n",
    "                             label=f'{ENM}-{model}')\n",
    "\n",
    "AUC = [CV_summaries_2[f'{n}-v1']['mean AUROC'][0] for n in x]\n",
    "OOB = [CV_summaries_2[f'{n}-v1']['mean OOB score'][0] for n in x]\n",
    "ax[0,0].plot(x, AUC, 'C8s--')\n",
    "ax[1,0].plot(x, OOB, 'C8s--', label='v1')\n",
    "\n",
    "ax[1,0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('local/figures/performances_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "featsets = {}\n",
    "featsets['full'] = [\n",
    "    'wtPSIC', 'ΔPSIC', 'SASA', 'MSF', \n",
    "    'effectiveness', 'sensitivity', 'stiffness', \n",
    "    'entropy', 'MI', 'BLOSUM']\n",
    "featsets['reduced'] = [\n",
    "    'wtPSIC', 'ΔPSIC', 'SASA', 'MSF', \n",
    "    'effectiveness', 'sensitivity', 'stiffness', 'BLOSUM']\n",
    "featsets['EVmut'] = featsets['full'] + ['EVmut-ΔE',]\n",
    "SEQ_feats = ['wtPSIC', 'ΔPSIC', 'BLOSUM', 'entropy', 'MI', 'EVmut-ΔE']\n",
    "\n",
    "ax[0,0].set_title('full classifier')\n",
    "ax[0,1].set_title('reduced classifier')\n",
    "ax[0,2].set_title('full+EVmutation classifier')\n",
    "ax[0,0].set_ylabel('sequence-based features')\n",
    "ax[1,0].set_ylabel('structure-based features')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([0, .31])\n",
    "    ax[1,j].set_ylim([0, .125])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "\n",
    "x = [1, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "for i, (version, featset) in enumerate(featsets.items()):\n",
    "    for j,f in enumerate(featset):\n",
    "        scheme = f'{n}-ANM-chain-{version}'\n",
    "        ss = [CV_summaries_2[scheme]['feat. importances'][0][j] for n in x]\n",
    "        if f in ['BLOSUM', 'SASA']:\n",
    "            m = 'kv-'\n",
    "        elif f == 'EVmut-DeltaE':\n",
    "            m = '^-'\n",
    "        else:\n",
    "            m = 'o-'\n",
    "        if f in SEQ_feats:\n",
    "            ax[0,i].plot(x, ss, m, label=f)\n",
    "        else:\n",
    "            ax[1,i].plot(x, ss, m, label=f)\n",
    "            \n",
    "for a in ax[0]:\n",
    "    a.legend(loc='upper right', ncol=2)\n",
    "for a in ax[1]:\n",
    "    a.legend(loc='lower right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('local/figures/feat_imp_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5,4))\n",
    "\n",
    "x = [1, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "y = np.array([CV_summaries_2[f'{n}-ANM-chain-full']['dataset size'] for n in x])\n",
    "h1 = ax.plot(x, y, '-o', label='dataset size')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "h2 = ax2.plot(x, [CV_summaries_2[f'{n}-ANM-chain-full']['dataset bias'] for n in x], \n",
    "              'C3-s', label='dataset bias')\n",
    "\n",
    "ax.set_xlabel('minimum PDB size (residues)')\n",
    "ax.set_ylabel('number of SAVs')\n",
    "ax2.set_ylabel('fraction of del. SAVs')\n",
    "\n",
    "ax.set_ylim((2000, 22000))\n",
    "ax2.set_ylim((.675, .765))\n",
    "\n",
    "ax.locator_params(nbins=5, axis='y')\n",
    "ax2.locator_params(nbins=7, axis='y')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('local/figures/dataset_size_vs_bias.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter optimization\n",
    "\n",
    "The main hyperparameters of a Random Forest classifier that can be tuned are:\n",
    "\n",
    "* the number of \"trees\" in the \"forest\"\n",
    "* the maximum number of features used for training a single tree.\n",
    "\n",
    "Based on the previous analysis, we will perform hyperparameter optimization only on the `'150-ANM-chain-v2'` classification scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this cell requires ~3 hours to complete\n",
    "\n",
    "if os.path.isfile('local/results/CV_summaries_3.pkl'):\n",
    "    CV_summaries_3 = pickle.load(open('local/results/CV_summaries_3.pkl', 'rb'))\n",
    "    print('A pickle containing precomputed results has been found. \\n'\n",
    "          'Please delete it if you wish to run the analysis again. \\n'\n",
    "          'Recovered output: \\n')\n",
    "    ! cat local/results/RF_optimization_3.log\n",
    "else:   \n",
    "    LOGGER.start('local/results/RF_optimization_3.log')\n",
    "    CV_summaries_3 = {}\n",
    "    \n",
    "    ID_subset = ID[ ID['PDB_length'] >= 150 ]\n",
    "    sel = ['SAV_coords', 'true_label'] + rd.DEFAULT_FEATSETS['full']\n",
    "    ID_subset = ID_subset[sel]\n",
    "    \n",
    "    for n_trees in [100, 500, 1000, 1500, 2000]:\n",
    "        for max_nfeats in range(2, len(featset)-1):\n",
    "\n",
    "            scheme = f'{n_trees}-{max_nfeats}'\n",
    "\n",
    "            LOGGER.info(f'PARAMETERS: {scheme}')\n",
    "\n",
    "            # create folder\n",
    "            folder = f'local/results/hyp_optimization-{scheme}'\n",
    "            os.mkdir(folder)\n",
    "\n",
    "            # run cross-validation\n",
    "            CV_summaries_3[scheme] = rd.RandomForestCV(\n",
    "                ID_subset, n_estimators=n_trees, max_features=max_nfeats)\n",
    "\n",
    "            # move figures into folder\n",
    "            for file in glob('*png'):\n",
    "                os.rename(file, os.path.join(folder, file))\n",
    "\n",
    "            LOGGER.info('')\n",
    "                    \n",
    "    # store all cross-validation results into a pickle\n",
    "    pickle.dump(CV_summaries_3, open('local/results/CV_summaries_3.pkl', 'wb'))\n",
    "\n",
    "    LOGGER.close('local/results/RF_optimization_3.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.5,3))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "x = [100, 500, 1000, 1500, 2000]\n",
    "\n",
    "for max_nfeats in range(2, len(rd.DEFAULT_FEATSETS['full'])-1):\n",
    "\n",
    "    summ = CV_summaries_3[f'{n}-{max_nfeats}']\n",
    "    \n",
    "    AUROC = np.array([summ['mean AUROC'][0] for n in x])\n",
    "    OOB = np.array([summ['mean OOB score'][0] for n in x])\n",
    "    if max_nfeats == 2:\n",
    "        AUROC_e = np.array([summ['mean AUROC'][1] for n in x])\n",
    "        OOB_e = np.array([summ['mean OOB score'][1] for n in x])\n",
    "        lw, mk = 1.5, 'o'\n",
    "    else:\n",
    "        AUROC_e, OOB_e = None, None\n",
    "        lw, mk = 1, '-'\n",
    "\n",
    "    ax1.errorbar(x, AUROC, yerr=AUROC_e, m=mk, lw=lw)\n",
    "    ax2.errorbar(x, OOB, yerr=OOB_e, m=mk, lw=lw, label=f'{max_nfeats}')\n",
    "\n",
    "ax1.set_xlabel('number of trees')\n",
    "ax1.set_ylabel('AUROC')\n",
    "ax2.set_xlabel('number of trees')\n",
    "ax2.set_ylabel('OOB score')\n",
    "\n",
    "ax1.set_ylim((0.845, 0.86))\n",
    "ax2.set_ylim((0.80, 0.815))\n",
    "ax1.locator_params(nbins=5, axis='y')\n",
    "ax2.locator_params(nbins=5, axis='y')\n",
    "\n",
    "lgd = fig.legend(title='n. features', bbox_to_anchor=(1, 0.94), \n",
    "                 loc='upper left', ncol=1)\n",
    "plt.tight_layout()\n",
    "fig.savefig('local/figures/hyp_optimization.png', bbox_extra_artists=(lgd,), \n",
    "            bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
