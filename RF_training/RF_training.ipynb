{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Random Forest classifiers\n",
    "\n",
    "In the following, we train different versions of the Rhapsody classifier and compare their accuracy. Each version is trained on the Integrated Dataset of human missense variants and evaluated through 10-fold cross-validation. \n",
    "\n",
    "More specifically, we considered:\n",
    "* different subsets of features,\n",
    "* different subsets of the training dataset,\n",
    "* different classifier's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here path to Rhapsody folder\n",
    "sys.path.insert(0, '/home/lponzoni/Scratch/028-RHAPSODY-git/rhapsody') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhapsody import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training dataset\n",
    "\n",
    "The Integrated Dataset used for training is made available as a NumPy structured array containing all precomputed features, as well as true labels and other info (e.g. PDB lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = pickle.load(open('Integrated_Dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SAV_coords',\n",
       " 'Uniprot2PDB',\n",
       " 'PDB_length',\n",
       " 'true_label',\n",
       " 'wt_PSIC',\n",
       " 'Delta_PSIC',\n",
       " 'SASA',\n",
       " 'Delta_SASA',\n",
       " 'BLOSUM',\n",
       " 'GNM_MSF-chain',\n",
       " 'GNM_MSF-reduced',\n",
       " 'GNM_effectiveness-chain',\n",
       " 'GNM_effectiveness-reduced',\n",
       " 'GNM_sensitivity-chain',\n",
       " 'GNM_sensitivity-reduced',\n",
       " 'ANM_MSF-chain',\n",
       " 'ANM_MSF-reduced',\n",
       " 'ANM_effectiveness-chain',\n",
       " 'ANM_effectiveness-reduced',\n",
       " 'ANM_sensitivity-chain',\n",
       " 'ANM_sensitivity-reduced',\n",
       " 'stiffness-chain',\n",
       " 'stiffness-reduced',\n",
       " 'MBS-chain',\n",
       " 'MBS-reduced',\n",
       " 'entropy',\n",
       " 'ranked_MI',\n",
       " 'EVmut-DeltaE_epist',\n",
       " 'EVmut-DeltaE_indep',\n",
       " 'EVmut-wt_aa_cons',\n",
       " 'EVmut-mut_aa_freq')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array structure\n",
    "ID.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Q96JB6 405 D A', 'Unable to map SAV to PDB', nan, 0, -0.924, 2.234, nan, nan, -2., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.469, 0.9847, -6.859, -2.213, 0.5493, 0.07133)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each entry can be accessed by indexing\n",
    "ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92108"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the training dataset\n",
    "len(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28010"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of entries with an associated PDB structure\n",
    "len(ID[~np.isnan(ID['PDB_length'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with various classification schemes\n",
    "\n",
    "We assess, through 10-fold cross-validation, the effect of considering different combinations of features and training datasets. \n",
    "In particular, we compare performances when using:\n",
    "* different subsets of the training dataset, obtained by selecting those cases where PDB structures were larger than *n* residues\n",
    "* 4 different feature sets, including one reproducing version 1 of the method (RAPSODY)\n",
    "* **GNM** vs **ANM** features\n",
    "* ENM features computed with and without the inclusion of *enviromental* effects (**reduced** vs **chain** model), i.e. the presence of other chains in the PDB structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Logging into file: results/RF_training.log\n",
      "@> Logging started at 2019-02-06 19:15:20.852476\n",
      "@> CLASSIFICATION SCHEME: 0-GNM-chain-v2\n",
      "@> 5955 out of 28010 cases ignored with missing features.\n",
      "@> CV iteration # 1:    ROC-AUC = 0.839   OOB score = 0.817\n",
      "@> CV iteration # 2:    ROC-AUC = 0.851   OOB score = 0.817\n",
      "@> CV iteration # 3:    ROC-AUC = 0.839   OOB score = 0.818\n",
      "@> CV iteration # 4:    ROC-AUC = 0.828   OOB score = 0.818\n",
      "@> CV iteration # 5:    ROC-AUC = 0.840   OOB score = 0.817\n",
      "@> CV iteration # 6:    ROC-AUC = 0.846   OOB score = 0.820\n",
      "@> CV iteration # 7:    ROC-AUC = 0.843   OOB score = 0.818\n",
      "@> CV iteration # 8:    ROC-AUC = 0.845   OOB score = 0.818\n",
      "@> CV iteration # 9:    ROC-AUC = 0.860   OOB score = 0.817\n",
      "@> CV iteration #10:    ROC-AUC = 0.851   OOB score = 0.819\n",
      "@> ------------------------------------------------------------\n",
      "@> Cross-validation summary:\n",
      "@> training dataset size:   22055\n",
      "@> fraction of positives:   0.731\n",
      "@> mean ROC-AUC:            0.844\n",
      "@> mean OOB score:          0.818\n",
      "@> optimal cutoff*:         0.731 +/- 0.025\n",
      "@> (* argmax of Youden's index)\n",
      "@> feature importances:\n",
      "@>                 wt_PSIC: 0.146\n",
      "@>              Delta_PSIC: 0.197\n",
      "@>                    SASA: 0.076\n",
      "@>           GNM_MSF-chain: 0.086\n",
      "@> GNM_effectiveness-chain: 0.083\n",
      "@>   GNM_sensitivity-chain: 0.082\n",
      "@>         stiffness-chain: 0.085\n",
      "@>                 entropy: 0.113\n",
      "@>               ranked_MI: 0.081\n",
      "@>                  BLOSUM: 0.050\n",
      "@> ------------------------------------------------------------\n",
      "@> Predictions distribution saved to predictions_distribution.png\n",
      "@> Pathogenicity plot saved to pathogenicity_prob.png\n",
      "@> ROC plot saved to ROC.png\n",
      "@> ------------------------------------------------------------\n",
      "@> Classifier training summary:\n",
      "@> mean OOB score:          0.819\n",
      "@> feature importances:\n",
      "@>                 wt_PSIC: 0.147\n",
      "@>              Delta_PSIC: 0.195\n",
      "@>                    SASA: 0.076\n",
      "@>           GNM_MSF-chain: 0.086\n",
      "@> GNM_effectiveness-chain: 0.082\n",
      "@>   GNM_sensitivity-chain: 0.082\n",
      "@>         stiffness-chain: 0.085\n",
      "@>                 entropy: 0.113\n",
      "@>               ranked_MI: 0.081\n",
      "@>                  BLOSUM: 0.050\n",
      "@> ------------------------------------------------------------\n",
      "@> Feat. importance plot saved to feat_importances.png\n",
      "@> \n",
      "@> CLASSIFICATION SCHEME: 0-GNM-chain-v2_noPfam\n",
      "@> 315 out of 28010 cases ignored with missing features.\n"
     ]
    }
   ],
   "source": [
    "# this cell requires ~3.5 hours to complete\n",
    "\n",
    "from prody import LOGGER\n",
    "LOGGER.start('results/RF_training.log')\n",
    "\n",
    "CV_summaries = {}\n",
    "\n",
    "if os.path.isfile('results/CV-summaries.pkl'):\n",
    "    print('A pickle containing precomputed training results have been found.')\n",
    "    print('Please delete it if you wish to run the training again.')\n",
    "else:\n",
    "    for min_num_res in [0, 100, 150, 200, 300, 400, 500, 600]:\n",
    "        # compute subset of the training dataset\n",
    "        ID_subset = ID[ ~np.isnan(ID['PDB_length']) ]\n",
    "        ID_subset = ID_subset[ ID_subset['PDB_length'] >= min_num_res ]\n",
    "        \n",
    "        # loop over different classification schemes\n",
    "        for ENM in ['GNM', 'ANM']:\n",
    "            for model in ['chain', 'reduced']:\n",
    "                for version in ['v2', 'v2_noPfam', 'v2_EVmut', 'v1']:\n",
    "\n",
    "                    # select feature set (+ true label)\n",
    "                    if version == 'v2':\n",
    "                        # full classifier\n",
    "                        featset = ['SAV_coords', 'true_label', \n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'entropy', 'ranked_MI', 'BLOSUM']\n",
    "                    elif version == 'v2_noPfam':\n",
    "                        # reduced classifier\n",
    "                        featset = ['SAV_coords', 'true_label',\n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'BLOSUM']\n",
    "                    elif version == 'v2_EVmut':\n",
    "                        # full classifier + EVmutation epistatic score\n",
    "                        featset = ['SAV_coords', 'true_label',\n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'entropy', 'ranked_MI', 'BLOSUM',\n",
    "                                   'EVmut-DeltaE_epist']\n",
    "                    elif version == 'v1' and ENM == 'GNM' and model == 'chain':\n",
    "                        # classifier as in version 1 of Rhapsody (RAPSODY)\n",
    "                        # NB: RAPSODY used a combination of GNM/ANM features, which\n",
    "                        # we reproduce here for the sake of comparison\n",
    "                        featset = ['SAV_coords', 'true_label', \n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   'GNM_MSF-chain', \n",
    "                                   'ANM_effectiveness-chain', \n",
    "                                   'ANM_sensitivity-chain',\n",
    "                                   'stiffness-chain']\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    if version == 'v1':\n",
    "                        scheme = f'{min_num_res}-v1'\n",
    "                    else:\n",
    "                        scheme = f'{min_num_res}-{ENM}-{model}-{version}'\n",
    "                        \n",
    "                    LOGGER.info(f'CLASSIFICATION SCHEME: {scheme}')\n",
    "\n",
    "                    # create folder\n",
    "                    folder = f'results/clsf_scheme-{scheme}'\n",
    "                    os.mkdir(folder)\n",
    "                    \n",
    "                    # train the classifier\n",
    "                    clsf = trainRFclassifier(ID_subset[featset])\n",
    "\n",
    "                    # store summary from cross-validation into a dictionary\n",
    "                    CV_summaries[scheme] = clsf['CV summary']\n",
    "\n",
    "                    # move figures into folder\n",
    "                    for file in glob('*png'):\n",
    "                        os.rename(file, os.path.join(folder, file))\n",
    "\n",
    "                    # we'll only keep classifiers trained with the 150 min_num_res requirement\n",
    "                    clsf_file = 'trained_classifier.pkl'\n",
    "                    if min_num_res == 150:\n",
    "                        os.rename(clsf_file, os.path.join(folder, clsf_file))\n",
    "                    else:\n",
    "                        os.remove(clsf_file)\n",
    "                    \n",
    "                    LOGGER.info('')\n",
    "                    \n",
    "    # store all cross-validation results into a pickle\n",
    "    pickle.dump(CV_summaries, open('results/CV_summaries.pkl', 'wb'))\n",
    "\n",
    "LOGGER.close('results/RF_training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover precomputed cross-validation results\n",
    "CV_summaries = pickle.load(open('results/CV_summaries.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "ax[0,0].set_title('RHAPSODY')\n",
    "ax[0,1].set_title('RHAPSODY w/o Pfam features')\n",
    "ax[0,2].set_title('RHAPSODY w/ EVmutation score')\n",
    "ax[0,0].set_ylabel('AUROC')\n",
    "ax[1,0].set_ylabel('OOB score')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([.80, .88])\n",
    "    ax[1,j].set_ylim([.80, .88])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "for ENM in ['GNM', 'ANM']:\n",
    "    for model in ['chain', 'reduced']:\n",
    "        for i, version in enumerate(['v2', 'v2_noPfam', 'v2_EVmut']):\n",
    "            scheme = f'{ENM}-{model}-{version}'\n",
    "            AUC = [CV_summaries[f'{n}-{scheme}']['mean ROC-AUC'] for n in x]\n",
    "            OOB = [CV_summaries[f'{n}-{scheme}']['mean OOB score'] for n in x]\n",
    "            ax[0,i].plot(x, AUC, 'o-', label=scheme)\n",
    "            ax[1,i].plot(x, OOB, 'o-', label=scheme)\n",
    "\n",
    "AUC = [CV_summaries[f'{n}-v1']['mean ROC-AUC'] for n in x]\n",
    "OOB = [CV_summaries[f'{n}-v1']['mean OOB score'] for n in x]\n",
    "ax[0,0].plot(x, AUC, 'v--', label='v1')\n",
    "ax[1,0].plot(x, OOB, 'v--', label='v1')\n",
    "\n",
    "ax[0,0].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/performances_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "featsets = {}\n",
    "featsets['v2'] = ['wt_PSIC', 'Delta_PSIC', 'SASA', 'MSF', \n",
    "                  'effectiveness', 'sensitivity', 'stiffness', \n",
    "                  'entropy', 'ranked_MI', 'BLOSUM']\n",
    "featsets['v2_noPfam'] = ['wt_PSIC', 'Delta_PSIC', 'SASA', 'MSF', \n",
    "                         'effectiveness', 'sensitivity', 'stiffness', 'BLOSUM']\n",
    "featsets['v2_EVmut'] = featsets['v2'] + ['EVmut-DeltaE',]\n",
    "SEQ_feats = ['wt_PSIC', 'Delta_PSIC', 'BLOSUM', 'entropy', 'ranked_MI', 'EVmut-DeltaE']\n",
    "\n",
    "ax[0,0].set_title('RHAPSODY')\n",
    "ax[0,1].set_title('RHAPSODY w/o Pfam features')\n",
    "ax[0,2].set_title('RHAPSODY w/ EVmutation score')\n",
    "ax[0,0].set_ylabel('sequence-based features')\n",
    "ax[1,0].set_ylabel('structure-based features')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([0, .3])\n",
    "    ax[1,j].set_ylim([0, .13])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "for i, (version, featset) in enumerate(featsets.items()):\n",
    "    for j,f in enumerate(featset):\n",
    "        ss = [CV_summaries[f'{n}-ANM-chain-{version}']['feat. importance'][j] for n in x]\n",
    "        if f in ['BLOSUM', 'SASA']:\n",
    "            m = 'kv-'\n",
    "        else:\n",
    "            m = 'o-'\n",
    "        if f in SEQ_feats:\n",
    "            ax[0,i].plot(x, ss, m, label=f)\n",
    "        else:\n",
    "            ax[1,i].plot(x, ss, m, label=f)\n",
    "            \n",
    "for a in ax[0]:\n",
    "    a.legend(loc='upper right', ncol=2)\n",
    "for a in ax[1]:\n",
    "    a.legend(loc='lower right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/feat_imp_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.5,3))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "y = np.array([CV_summaries[f'{n}-ANM-chain-v2']['dataset size'] for n in x])\n",
    "ax1.plot(x, y, 'o-')\n",
    "ax1.set_xlabel('minimum number of residues')\n",
    "ax1.set_ylabel('number of SAVs')\n",
    "\n",
    "y2 = np.array([CV_summaries[f'{n}-ANM-chain-v2']['dataset bias'] for n in x])\n",
    "ax2.plot(x, y2, 'o-')\n",
    "ax2.set_xlabel('minimum number of residues')\n",
    "ax2.set_ylabel('fraction of del. SAVs')\n",
    "ax2.set_ylim((0.7, 0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/stats.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for version in ['v2', 'v2_noPfam', 'v2_EVmut', 'v1']:\n",
    "    for min_num_res in [0, 100, 150, 200, 300, 400, 500, 600]:\n",
    "        for ENM in ['GNM', 'ANM']:\n",
    "            for model in ['chain', 'reduced']:\n",
    "\n",
    "                if version == 'v1' and ENM == 'GNM' and model == 'chain':\n",
    "                    scheme = f'{min_num_res}-v1'\n",
    "                elif version != 'v1':\n",
    "                    scheme = f'{min_num_res}-{ENM}-{model}-{version}'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                s = CV_summaries[scheme]\n",
    "               \n",
    "                print(f'{version:<9} {min_num_res:3} {ENM} {model:7}:  ',\n",
    "                      'size = {:5}  '.format(s['dataset size']),\n",
    "                      'AUROC = {:5.3f}  '.format(s['mean ROC-AUC']),\n",
    "                      'OOB = {:5.3f}'.format(s['mean OOB score']) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
