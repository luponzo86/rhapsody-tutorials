{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Random Forest classifiers\n",
    "\n",
    "In the following, we train different versions of the Rhapsody classifier and compare their accuracy. Each version is trained on the Integrated Dataset of human missense variants and evaluated through 10-fold cross-validation. \n",
    "\n",
    "More specifically, we considered:\n",
    "* different subsets of features,\n",
    "* different subsets of the training dataset,\n",
    "* different classifier's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here path to Rhapsody folder\n",
    "sys.path.insert(0, '/home/lponzoni/Scratch/028-RHAPSODY-git/rhapsody') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhapsody import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training dataset\n",
    "\n",
    "The Integrated Dataset used for training is made available as a NumPy structured array containing all precomputed features, as well as true labels and other info (e.g. PDB lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = pickle.load(open('Integrated_Dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array structure\n",
    "ID.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each entry can be accessed by indexing\n",
    "ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the training dataset\n",
    "len(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries with an associated PDB structure\n",
    "len(ID[~np.isnan(ID['PDB_length'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with various classification schemes\n",
    "\n",
    "We assess, through 10-fold cross-validation, the effect of considering different combinations of features and training datasets. \n",
    "In particular, we compare performances when using:\n",
    "* different subsets of the training dataset, obtained by selecting those cases where PDB structures were larger than *n* residues\n",
    "* 4 different feature sets, including one reproducing version 1 of the method (RAPSODY)\n",
    "* **GNM** vs **ANM** features\n",
    "* ENM features computed with and without the inclusion of *enviromental* effects (**reduced** vs **chain** model), i.e. the presence of other chains in the PDB structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this cell requires ~3.5 hours to complete\n",
    "\n",
    "from prody import LOGGER\n",
    "LOGGER.start('results/RF_training.log')\n",
    "\n",
    "CV_summaries = {}\n",
    "\n",
    "if os.path.isfile('results/CV-summaries.pkl'):\n",
    "    print('A pickle containing precomputed training results have been found.')\n",
    "    print('Please delete it if you wish to run the training again.')\n",
    "else:\n",
    "    for min_num_res in [0, 100, 150, 200, 300, 400, 500, 600]:\n",
    "        # compute subset of the training dataset\n",
    "        ID_subset = ID[ ~np.isnan(ID['PDB_length']) ]\n",
    "        ID_subset = ID_subset[ ID_subset['PDB_length'] >= min_num_res ]\n",
    "        \n",
    "        # loop over different classification schemes\n",
    "        for ENM in ['GNM', 'ANM']:\n",
    "            for model in ['chain', 'reduced']:\n",
    "                for version in ['v2', 'v2_noPfam', 'v2_EVmut', 'v1']:\n",
    "\n",
    "                    # select feature set (+ true label)\n",
    "                    if version == 'v2':\n",
    "                        # full classifier\n",
    "                        featset = ['SAV_coords', 'true_label', \n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'entropy', 'ranked_MI', 'BLOSUM']\n",
    "                    elif version == 'v2_noPfam':\n",
    "                        # reduced classifier\n",
    "                        featset = ['SAV_coords', 'true_label',\n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'BLOSUM']\n",
    "                    elif version == 'v2_EVmut':\n",
    "                        # full classifier + EVmutation epistatic score\n",
    "                        featset = ['SAV_coords', 'true_label',\n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'entropy', 'ranked_MI', 'BLOSUM',\n",
    "                                   'EVmut-DeltaE_epist']\n",
    "                    elif version == 'v1' and ENM == 'GNM' and model == 'chain':\n",
    "                        # classifier as in version 1 of Rhapsody (RAPSODY)\n",
    "                        # NB: RAPSODY used a combination of GNM/ANM features, which\n",
    "                        # we reproduce here for the sake of comparison\n",
    "                        featset = ['SAV_coords', 'true_label', \n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   'GNM_MSF-chain', \n",
    "                                   'ANM_effectiveness-chain', \n",
    "                                   'ANM_sensitivity-chain',\n",
    "                                   'stiffness-chain']\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    if version == 'v1':\n",
    "                        scheme = f'{min_num_res}-v1'\n",
    "                    else:\n",
    "                        scheme = f'{min_num_res}-{ENM}-{model}-{version}'\n",
    "                        \n",
    "                    LOGGER.info(f'CLASSIFICATION SCHEME: {scheme}')\n",
    "\n",
    "                    # create folder\n",
    "                    folder = f'results/clsf_scheme-{scheme}'\n",
    "                    os.mkdir(folder)\n",
    "                    \n",
    "                    # train the classifier\n",
    "                    clsf = trainRFclassifier(ID_subset[featset])\n",
    "\n",
    "                    # store summary from cross-validation into a dictionary\n",
    "                    CV_summaries[scheme] = clsf['CV summary']\n",
    "\n",
    "                    # move figures into folder\n",
    "                    for file in glob('*png'):\n",
    "                        os.rename(file, os.path.join(folder, file))\n",
    "\n",
    "                    # we'll only keep classifiers trained with the 150 min_num_res requirement\n",
    "                    clsf_file = 'trained_classifier.pkl'\n",
    "                    if min_num_res == 150:\n",
    "                        os.rename(clsf_file, os.path.join(folder, clsf_file))\n",
    "                    else:\n",
    "                        os.remove(clsf_file)\n",
    "                    \n",
    "                    LOGGER.info('')\n",
    "                    \n",
    "    # store all cross-validation results into a pickle\n",
    "    pickle.dump(CV_summaries, open('results/CV_summaries.pkl', 'wb'))\n",
    "\n",
    "LOGGER.close('results/RF_training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover precomputed cross-validation results\n",
    "CV_summaries = pickle.load(open('results/CV_summaries.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "ax[0,0].set_title('RHAPSODY')\n",
    "ax[0,1].set_title('RHAPSODY w/o Pfam features')\n",
    "ax[0,2].set_title('RHAPSODY w/ EVmutation score')\n",
    "ax[0,0].set_ylabel('AUROC')\n",
    "ax[1,0].set_ylabel('OOB score')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([.80, .88])\n",
    "    ax[1,j].set_ylim([.80, .88])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "for ENM in ['GNM', 'ANM']:\n",
    "    for model in ['chain', 'reduced']:\n",
    "        for i, version in enumerate(['v2', 'v2_noPfam', 'v2_EVmut']):\n",
    "            scheme = f'{ENM}-{model}-{version}'\n",
    "            AUC = [CV_summaries[f'{n}-{scheme}']['mean ROC-AUC'] for n in x]\n",
    "            OOB = [CV_summaries[f'{n}-{scheme}']['mean OOB score'] for n in x]\n",
    "            ax[0,i].plot(x, AUC, 'o-', label=scheme)\n",
    "            ax[1,i].plot(x, OOB, 'o-', label=scheme)\n",
    "\n",
    "AUC = [CV_summaries[f'{n}-v1']['mean ROC-AUC'] for n in x]\n",
    "OOB = [CV_summaries[f'{n}-v1']['mean OOB score'] for n in x]\n",
    "ax[0,0].plot(x, AUC, 'v--', label='v1')\n",
    "ax[1,0].plot(x, OOB, 'v--', label='v1')\n",
    "\n",
    "ax[0,0].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/performances_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "featsets = {}\n",
    "featsets['v2'] = ['wt_PSIC', 'Delta_PSIC', 'SASA', 'MSF', \n",
    "                  'effectiveness', 'sensitivity', 'stiffness', \n",
    "                  'entropy', 'ranked_MI', 'BLOSUM']\n",
    "featsets['v2_noPfam'] = ['wt_PSIC', 'Delta_PSIC', 'SASA', 'MSF', \n",
    "                         'effectiveness', 'sensitivity', 'stiffness', 'BLOSUM']\n",
    "featsets['v2_EVmut'] = featsets['v2'] + ['EVmut-DeltaE',]\n",
    "SEQ_feats = ['wt_PSIC', 'Delta_PSIC', 'BLOSUM', 'entropy', 'ranked_MI', 'EVmut-DeltaE']\n",
    "\n",
    "ax[0,0].set_title('RHAPSODY')\n",
    "ax[0,1].set_title('RHAPSODY w/o Pfam features')\n",
    "ax[0,2].set_title('RHAPSODY w/ EVmutation score')\n",
    "ax[0,0].set_ylabel('sequence-based features')\n",
    "ax[1,0].set_ylabel('structure-based features')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([0, .3])\n",
    "    ax[1,j].set_ylim([0, .13])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "for i, (version, featset) in enumerate(featsets.items()):\n",
    "    for j,f in enumerate(featset):\n",
    "        ss = [CV_summaries[f'{n}-ANM-chain-{version}']['feat. importance'][j] for n in x]\n",
    "        if f in ['BLOSUM', 'SASA']:\n",
    "            m = 'kv-'\n",
    "        else:\n",
    "            m = 'o-'\n",
    "        if f in SEQ_feats:\n",
    "            ax[0,i].plot(x, ss, m, label=f)\n",
    "        else:\n",
    "            ax[1,i].plot(x, ss, m, label=f)\n",
    "            \n",
    "for a in ax[0]:\n",
    "    a.legend(loc='upper right', ncol=2)\n",
    "for a in ax[1]:\n",
    "    a.legend(loc='lower right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/feat_imp_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.5,3))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "y = np.array([CV_summaries[f'{n}-ANM-chain-v2']['dataset size'] for n in x])\n",
    "ax1.plot(x, y, 'o-')\n",
    "ax1.set_xlabel('minimum number of residues')\n",
    "ax1.set_ylabel('number of SAVs')\n",
    "\n",
    "y2 = np.array([CV_summaries[f'{n}-ANM-chain-v2']['dataset bias'] for n in x])\n",
    "ax2.plot(x, y2, 'o-')\n",
    "ax2.set_xlabel('minimum number of residues')\n",
    "ax2.set_ylabel('fraction of del. SAVs')\n",
    "ax2.set_ylim((0.7, 0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/stats.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for version in ['v2', 'v2_noPfam', 'v2_EVmut', 'v1']:\n",
    "    for min_num_res in [0, 100, 150, 200, 300, 400, 500, 600]:\n",
    "        for ENM in ['GNM', 'ANM']:\n",
    "            for model in ['chain', 'reduced']:\n",
    "\n",
    "                if version == 'v1' and ENM == 'GNM' and model == 'chain':\n",
    "                    scheme = f'{min_num_res}-v1'\n",
    "                elif version != 'v1':\n",
    "                    scheme = f'{min_num_res}-{ENM}-{model}-{version}'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                s = CV_summaries[scheme]\n",
    "               \n",
    "                print(f'{version:<9} {min_num_res:3} {ENM} {model:7}:  ',\n",
    "                      'size = {:5}  '.format(s['dataset size']),\n",
    "                      'AUROC = {:5.3f}  '.format(s['mean ROC-AUC']),\n",
    "                      'OOB = {:5.3f}'.format(s['mean OOB score']) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
