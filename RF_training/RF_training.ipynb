{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Random Forest classifiers\n",
    "\n",
    "In the following, we train different versions of the Rhapsody classifier and compare their accuracy. Each version is trained on the Integrated Dataset of human missense variants and evaluated through 10-fold cross-validation. \n",
    "\n",
    "More specifically, we considered:\n",
    "* different subsets of features,\n",
    "* different subsets of the training dataset,\n",
    "* different classifier's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here path to Rhapsody folder\n",
    "sys.path.insert(0, '/home/lponzoni/Scratch/028-RHAPSODY-git/rhapsody') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhapsody import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training dataset\n",
    "\n",
    "The Integrated Dataset used for training is made available as a NumPy structured array containing all precomputed features, as well as true labels and other info (e.g. PDB lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = pickle.load(open('Integrated_Dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SAV_coords',\n",
       " 'Uniprot2PDB',\n",
       " 'PDB_length',\n",
       " 'true_label',\n",
       " 'wt_PSIC',\n",
       " 'Delta_PSIC',\n",
       " 'SASA',\n",
       " 'Delta_SASA',\n",
       " 'BLOSUM',\n",
       " 'GNM_MSF-chain',\n",
       " 'GNM_MSF-reduced',\n",
       " 'GNM_effectiveness-chain',\n",
       " 'GNM_effectiveness-reduced',\n",
       " 'GNM_sensitivity-chain',\n",
       " 'GNM_sensitivity-reduced',\n",
       " 'ANM_MSF-chain',\n",
       " 'ANM_MSF-reduced',\n",
       " 'ANM_effectiveness-chain',\n",
       " 'ANM_effectiveness-reduced',\n",
       " 'ANM_sensitivity-chain',\n",
       " 'ANM_sensitivity-reduced',\n",
       " 'stiffness-chain',\n",
       " 'stiffness-reduced',\n",
       " 'MBS-chain',\n",
       " 'MBS-reduced',\n",
       " 'entropy',\n",
       " 'ranked_MI',\n",
       " 'EVmut-DeltaE_epist',\n",
       " 'EVmut-DeltaE_indep',\n",
       " 'EVmut-wt_aa_cons',\n",
       " 'EVmut-mut_aa_freq')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array structure\n",
    "ID.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Q96JB6 405 D A', 'Unable to map SAV to PDB', nan, 0, -0.924, 2.234, nan, nan, -2., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.469, 0.9847, -6.859, -2.213, 0.5493, 0.07133)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each entry can be accessed by indexing\n",
    "ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92108"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the training dataset\n",
    "len(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28010"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of entries with an associated PDB structure\n",
    "len(ID[~np.isnan(ID['PDB_length'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with various classification schemes\n",
    "\n",
    "We assess, through 10-fold cross-validation, the effect of considering different combinations of features and training datasets. \n",
    "In particular, we compare performances when using:\n",
    "* different subsets of the training dataset, obtained by selecting those cases where PDB structures were larger than *n* residues\n",
    "* 4 different feature sets, including one reproducing version 1 of the method (RAPSODY)\n",
    "* **GNM** vs **ANM** features\n",
    "* ENM features computed with and without the inclusion of *enviromental* effects (**reduced** vs **chain** model), i.e. the presence of other chains in the PDB structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('RF_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Logging into file: RF_training.log\n",
      "@> Logging started at 2019-02-05 13:11:30.529657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION SCHEME: 0-GNM-chain-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> 5955 out of 28010 cases ignored with missing features.\n",
      "@> CV iteration # 1:    ROC-AUC = 0.840   OOB score = 0.817\n",
      "@> CV iteration # 2:    ROC-AUC = 0.852   OOB score = 0.817\n",
      "@> CV iteration # 3:    ROC-AUC = 0.839   OOB score = 0.817\n",
      "@> CV iteration # 4:    ROC-AUC = 0.827   OOB score = 0.818\n",
      "@> CV iteration # 5:    ROC-AUC = 0.841   OOB score = 0.817\n",
      "@> CV iteration # 6:    ROC-AUC = 0.846   OOB score = 0.820\n",
      "@> CV iteration # 7:    ROC-AUC = 0.842   OOB score = 0.819\n",
      "@> CV iteration # 8:    ROC-AUC = 0.845   OOB score = 0.817\n",
      "@> CV iteration # 9:    ROC-AUC = 0.859   OOB score = 0.818\n",
      "@> CV iteration #10:    ROC-AUC = 0.852   OOB score = 0.819\n",
      "@> ------------------------------------------------------------\n",
      "@> Cross-validation summary:\n",
      "@> training dataset size:   22055\n",
      "@> fraction of positives:   0.731\n",
      "@> mean ROC-AUC:            0.844\n",
      "@> mean OOB score:          0.818\n",
      "@> optimal cutoff*:         0.720 +/- 0.023\n",
      "@> (* argmax of Youden's index)\n",
      "@> feature importances:\n",
      "@>                 wt_PSIC: 0.147\n",
      "@>              Delta_PSIC: 0.198\n",
      "@>                    SASA: 0.076\n",
      "@>           GNM_MSF-chain: 0.086\n",
      "@> GNM_effectiveness-chain: 0.083\n",
      "@>   GNM_sensitivity-chain: 0.082\n",
      "@>         stiffness-chain: 0.085\n",
      "@>                 entropy: 0.113\n",
      "@>               ranked_MI: 0.081\n",
      "@>                  BLOSUM: 0.050\n",
      "@> ------------------------------------------------------------\n",
      "@> Predictions distribution saved to predictions_distribution.png\n",
      "@> Pathogenicity plot saved to pathogenicity_prob.png\n",
      "@> ROC plot saved to ROC.png\n",
      "@> ------------------------------------------------------------\n",
      "@> Classifier training summary:\n",
      "@> mean OOB score:          0.818\n",
      "@> feature importances:\n",
      "@>                 wt_PSIC: 0.145\n",
      "@>              Delta_PSIC: 0.200\n",
      "@>                    SASA: 0.075\n",
      "@>           GNM_MSF-chain: 0.086\n",
      "@> GNM_effectiveness-chain: 0.083\n",
      "@>   GNM_sensitivity-chain: 0.083\n",
      "@>         stiffness-chain: 0.085\n",
      "@>                 entropy: 0.112\n",
      "@>               ranked_MI: 0.081\n",
      "@>                  BLOSUM: 0.050\n",
      "@> ------------------------------------------------------------\n",
      "@> Feat. importance plot saved to feat_importances.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION SCHEME: 0-GNM-chain-v2_noPfam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> 315 out of 28010 cases ignored with missing features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-26ed751892c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;31m# train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mclsf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainRFclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;31m# store summary from cross-validation into a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scratch/028-RHAPSODY-git/rhapsody/rhapsody/RFtraining.py\u001b[0m in \u001b[0;36mtrainRFclassifier\u001b[0;34m(feat_matrix, n_estimators, max_features, pickle_name, feat_imp_fig, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# calculate optimal Youden cutoff through CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     CV_summary = RandomForestCV(X, y, n_estimators=n_estimators,\n\u001b[0;32m--> 191\u001b[0;31m                  max_features=max_features, feature_names=featset, **kwargs)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# train a classifier on the whole dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scratch/028-RHAPSODY-git/rhapsody/rhapsody/RFtraining.py\u001b[0m in \u001b[0;36mRandomForestCV\u001b[0;34m(X, y, n_estimators, max_features, n_splits, ROC_fig, feature_names, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0my_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# train Random Forest classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;31m# calculate probabilities over decision trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this cell requires ~3.5 hours to complete\n",
    "\n",
    "from prody import LOGGER\n",
    "LOGGER.start('RF_training.log')\n",
    "\n",
    "CV_summaries = {}\n",
    "\n",
    "if os.path.isfile('RF_training/CV-summaries.pkl'):\n",
    "    print('A pickle containing precomputed training results have been found.')\n",
    "    print('Please delete it if you wish to run the training again.')\n",
    "else:\n",
    "    for min_num_res in [0, 100, 150, 200, 300, 400, 500, 600]:\n",
    "        # compute subset of the training dataset\n",
    "        ID_subset = ID[ ~np.isnan(ID['PDB_length']) ]\n",
    "        ID_subset = ID_subset[ ID_subset['PDB_length'] >= min_num_res ]\n",
    "        \n",
    "        # loop over different classification schemes\n",
    "        for ENM in ['GNM', 'ANM']:\n",
    "            for model in ['chain', 'reduced']:\n",
    "                for version in ['v2', 'v2_noPfam', 'v2_EVmut', 'v1']:\n",
    "\n",
    "                    # select feature set (+ true label)\n",
    "                    if version == 'v2':\n",
    "                        # full classifier\n",
    "                        featset = ['true_label', \n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'entropy', 'ranked_MI', 'BLOSUM']\n",
    "                    elif version == 'v2_noPfam':\n",
    "                        # reduced classifier\n",
    "                        featset = ['true_label',\n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'BLOSUM']\n",
    "                    elif version == 'v2_EVmut':\n",
    "                        # full classifier + EVmutation epistatic score\n",
    "                        featset = ['true_label',\n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   f'{ENM}_MSF-{model}',\n",
    "                                   f'{ENM}_effectiveness-{model}',\n",
    "                                   f'{ENM}_sensitivity-{model}',\n",
    "                                   f'stiffness-{model}',\n",
    "                                   'entropy', 'ranked_MI', 'BLOSUM',\n",
    "                                   'EVmut-DeltaE_epist']\n",
    "                    elif version == 'v1' and ENM == 'GNM' and model == 'chain':\n",
    "                        # classifier as in version 1 of Rhapsody (RAPSODY)\n",
    "                        # NB: RAPSODY used a combination of GNM/ANM features, which\n",
    "                        # we reproduce here for the sake of comparison\n",
    "                        featset = ['true_label', \n",
    "                                   'wt_PSIC', 'Delta_PSIC', 'SASA', \n",
    "                                   'GNM_MSF-chain', \n",
    "                                   'ANM_effectiveness-chain', \n",
    "                                   'ANM_sensitivity-chain',\n",
    "                                   'stiffness-chain']\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    if version == 'v1':\n",
    "                        scheme = f'{min_num_res}-v1'\n",
    "                    else:\n",
    "                        scheme = f'{min_num_res}-{ENM}-{model}-{version}'\n",
    "                    print(f'CLASSIFICATION SCHEME: {scheme}')                        \n",
    "\n",
    "                    # create folder\n",
    "                    folder = f'RF_training/clsf_scheme-{scheme}'\n",
    "                    os.mkdir(folder)\n",
    "                    \n",
    "                    # train the classifier\n",
    "                    clsf = trainRFclassifier(ID_subset[featset])\n",
    "\n",
    "                    # store summary from cross-validation into a dictionary\n",
    "                    CV_summaries[scheme] = clsf['CV summary']\n",
    "\n",
    "                    # move figures into folder\n",
    "                    for file in glob('*png'):\n",
    "                        os.rename(file, os.path.join(folder, file))\n",
    "\n",
    "                    # we'll only keep classifiers trained with the 150 min_num_res requirement\n",
    "                    clsf_file = 'trained_classifier.pkl'\n",
    "                    if min_num_res == 150:\n",
    "                        os.rename(clsf_file, os.path.join(folder, clsf_file))\n",
    "                    else:\n",
    "                        os.remove(clsf_file)\n",
    "                    \n",
    "    # store all cross-validation results into a pickle\n",
    "    pickle.dump(CV_summaries, open('RF_training/CV-summaries.pkl', 'wb'))\n",
    "\n",
    "LOGGER.close('RF_training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover precomputed cross-validation results\n",
    "CV_summaries = pickle.load(open('RF_training/CV_summaries.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "ax[0,0].set_title('RHAPSODY')\n",
    "ax[0,1].set_title('RHAPSODY w/o Pfam features')\n",
    "ax[0,2].set_title('RHAPSODY w/ EVmutation score')\n",
    "ax[0,0].set_ylabel('AUROC')\n",
    "ax[1,0].set_ylabel('OOB score')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([.80, .88])\n",
    "    ax[1,j].set_ylim([.80, .88])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "for ENM in ['GNM', 'ANM']:\n",
    "    for model in ['chain', 'reduced']:\n",
    "        for i, version in enumerate(['v2', 'v2_noPfam', 'v2_EVmut']):\n",
    "            scheme = f'{ENM}-{model}-{version}'\n",
    "            AUC = [CV_summaries[f'{n}-{scheme}']['mean ROC-AUC'] for n in x]\n",
    "            OOB = [CV_summaries[f'{n}-{scheme}']['mean OOB score'] for n in x]\n",
    "            ax[0,i].plot(x, AUC, 'o-', label=scheme)\n",
    "            ax[1,i].plot(x, OOB, 'o-', label=scheme)\n",
    "\n",
    "AUC = [CV_summaries[f'{n}-v1']['mean ROC-AUC'] for n in x]\n",
    "OOB = [CV_summaries[f'{n}-v1']['mean OOB score'] for n in x]\n",
    "ax[0,0].plot(x, AUC, 'v--', label='v1')\n",
    "ax[1,0].plot(x, OOB, 'v--', label='v1')\n",
    "\n",
    "ax[0,0].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/performances_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(12,7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "featsets = {}\n",
    "featsets['v2'] = ['wt_PSIC', 'Delta_PSIC', 'SASA', 'MSF', \n",
    "                  'effectiveness', 'sensitivity', 'stiffness', \n",
    "                  'entropy', 'ranked_MI', 'BLOSUM']\n",
    "featsets['v2_noPfam'] = ['wt_PSIC', 'Delta_PSIC', 'SASA', 'MSF', \n",
    "                         'effectiveness', 'sensitivity', 'stiffness', 'BLOSUM']\n",
    "featsets['v2_EVmut'] = featsets['v2'] + ['EVmut-DeltaE',]\n",
    "SEQ_feats = ['wt_PSIC', 'Delta_PSIC', 'BLOSUM', 'entropy', 'ranked_MI', 'EVmut-DeltaE']\n",
    "\n",
    "ax[0,0].set_title('RHAPSODY')\n",
    "ax[0,1].set_title('RHAPSODY w/o Pfam features')\n",
    "ax[0,2].set_title('RHAPSODY w/ EVmutation score')\n",
    "ax[0,0].set_ylabel('sequence-based features')\n",
    "ax[1,0].set_ylabel('structure-based features')\n",
    "for j in range(3):\n",
    "    ax[0,j].set_ylim([0, .3])\n",
    "    ax[1,j].set_ylim([0, .13])\n",
    "    ax[1,j].set_xlabel('minimum number of residues')\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "for i, (version, featset) in enumerate(featsets.items()):\n",
    "    for j,f in enumerate(featset):\n",
    "        ss = [CV_summaries[f'{n}-ANM-chain-{version}']['feat. importance'][j] for n in x]\n",
    "        if f in ['BLOSUM', 'SASA']:\n",
    "            m = 'kv-'\n",
    "        else:\n",
    "            m = 'o-'\n",
    "        if f in SEQ_feats:\n",
    "            ax[0,i].plot(x, ss, m, label=f)\n",
    "        else:\n",
    "            ax[1,i].plot(x, ss, m, label=f)\n",
    "            \n",
    "for a in ax[0]:\n",
    "    a.legend(loc='upper right', ncol=2)\n",
    "for a in ax[1]:\n",
    "    a.legend(loc='lower right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/feat_imp_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.5,3))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "x = [0, 100, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "y = np.array([CV_summaries[f'{n}-ANM-chain-v2']['dataset size'] for n in x])\n",
    "ax1.plot(x, y, 'o-')\n",
    "ax1.set_xlabel('minimum number of residues')\n",
    "ax1.set_ylabel('number of SAVs')\n",
    "\n",
    "y2 = np.array([CV_summaries[f'{n}-ANM-chain-v2']['dataset bias'] for n in x])\n",
    "ax2.plot(x, y2, 'o-')\n",
    "ax2.set_xlabel('minimum number of residues')\n",
    "ax2.set_ylabel('fraction of del. SAVs')\n",
    "ax2.set_ylim((0.7, 0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/stats.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for version in ['v2', 'v2_noPfam', 'v2_EVmut', 'v1']:\n",
    "    for min_num_res in [0, 100, 150, 200, 300, 400, 500, 600]:\n",
    "        for ENM in ['GNM', 'ANM']:\n",
    "            for model in ['chain', 'reduced']:\n",
    "\n",
    "                if version == 'v1' and ENM == 'GNM' and model == 'chain':\n",
    "                    scheme = f'{min_num_res}-v1'\n",
    "                elif version != 'v1':\n",
    "                    scheme = f'{min_num_res}-{ENM}-{model}-{version}'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                s = CV_summaries[scheme]\n",
    "               \n",
    "                print(f'{version:<9} {min_num_res:3} {ENM} {model:7}:  ',\n",
    "                      'size = {:5}  '.format(s['dataset size']),\n",
    "                      'AUROC = {:5.3f}  '.format(s['mean ROC-AUC']),\n",
    "                      'OOB = {:5.3f}'.format(s['mean OOB score']) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
